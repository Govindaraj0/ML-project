This project implements a Gradient Boosting Machine (GBM) from scratch using NumPy.

Objectives:
- Implement Decision Tree Regressor
- Implement Gradient Boosting
- Understand MSE gradient
- Compare with sklearn

Algorithms Used:
- Decision Tree Regression
- Gradient Boosting
- Mean Squared Error optimization

Workflow:
1. Dataset generation with nonlinear features
2. Train base tree
3. Compute residuals
4. Fit new trees iteratively
5. Update prediction
6. Compare performance

Evaluation:
Custom model compared with sklearn GradientBoostingRegressor
Metrics used: Mean Squared Error and training time.

Unit testing included for prediction validation.
Mathematical derivation of gradient boosting with MSE included in report.txt
